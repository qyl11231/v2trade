下面是我们团队给你的**“指标模块重构整体任务清单（可直接拆 Jira / Cursor 开干）”**。我会按**目标→方案裁决→任务分层→每项 DoD（验收口径）**来写，并且明确哪些是“保留”、哪些是“删除/替换”。

> 背景依据：现有指标模块把 **BarClosedEvent 作为唯一计算触发源**，并且“订阅→自动计算→写 value / 写 calc_log”是主链路；产品规范也强调“按周期计算并持久化、不可根据策略参数算指标、不可 Tick 级实时指标”。你现在的重构目标是：**保留 K 线闭合订阅（维护输入），取消“每根 K 线自动算订阅指标”的实时计算（因为参数用户自定义，算出来没人用），改为谁用谁算 + 高效缓存。**

---

## Step 1：重构目标与方案裁决（先写进需求基线）

### 目标（你定的）

1. **保留 BarClosedEvent 订阅**：继续维护 BarSeries / K 线窗口（输入事实流必须稳定）。
2. **取消订阅触发的实时计算**：不再“每根 bar close 计算所有订阅指标并落库”。现有链路是“监听→查订阅→算→写库”。
3. **指标参数支持用户自定义表单**：指标定义必须提供 param_schema。现有表设计里 definition 已有 `param_schema/return_schema/engine` 等元数据。
4. **谁用谁调用**：策略大脑/回测/风控/页面查询按需调用 evaluate（支持 batch），并且要“高效率”。

### 方案裁决（我们团队建议你冻结成一句话）

> **“订阅保留用于 K 线输入维护；指标值默认不落库；按需计算 + 同参缓存 + batch 评估；保留 calc_log 作为审计/性能可观测。”**（calc_log 现有设计本就适合审计与性能分析）

---

## Step 2：整体任务清单（按模块拆）

# A. 产品/契约层任务（先改口径，否则开发会互相打架）

### A1. 更新《指标模块产品规范》（范围变更）

* **把“必须：按周期计算并持久化指标结果”改成“双模式”**：

    * FACT 模式（平台白名单/系统指标）：仍可按周期计算并持久化
    * RUNTIME 模式（用户自定义参数）：默认不持久化，只按需计算
      现有规范写了“必须持久化、不可根据策略参数算指标”——这条必须修订，否则你新架构与规范冲突。
* DoD：产品规范里明确：

    * 哪些指标/哪些场景允许落库
    * 用户自定义参数指标：只缓存/不落库（默认）
    * Tick 级实时指标仍然不做（保持原则）

### A2. 指标定义（definition）能力补齐：支持“表单渲染 + 后端校验”

* definition 中 `param_schema/return_schema/min_required_bars/supported_timeframes` 继续保留
* 新增/明确（如果表里没有就加字段或放 schema JSON 里）：

    * `data_source`: BAR / TICK / SIGNAL / MIXED
    * `eval_mode`: FACT_PERSISTED / RUNTIME_ON_DEMAND
    * `param_limits`: lookback 上限、窗口上限等（防止用户把资源打爆）
* DoD：前端能仅靠 definition 拉起表单，后端能按 schema 校验 params（必填、范围、枚举）。

---

# B. 事件与数据输入层（保留订阅，但只维护输入）

### B1. 保留 BarClosedEvent 订阅入口，但“降级职责”

* 现状：BarClosedEvent 是“唯一计算触发源”
* 重构：BarClosedEvent 只做：

    * BarSeriesManager append 新 bar
    * 幂等处理（同 bar_time 重复忽略，现有阶段验收点就有这个要求）
* DoD：

    * BarSeriesManager 在 5m/15m/30m/1h/4h 这些周期继续维护 365 根窗口（现状约定）
    * **不再触发任何“按订阅计算指标”流程**

### B2. 统一 bar_time 语义（必须保持 bar_close_time UTC）

* 现有设计已经强调 bar_time 是收盘时间 UTC
* DoD：所有 evaluate(context) 的 `asOfBarTime` 都用 bar_close_time 语义，避免回放/对账混乱。

---

# C. 指标运行时（核心：谁用谁算）

### C1. 新增统一执行入口：IndicatorEvaluateService

* API/SDK 形式二选一（建议都做，API 给 UI/外部，SDK 给策略运行时）：

    * `evaluate(indicatorCode, version, params, context) -> output`
    * `evaluateBatch(context, requests[]) -> outputs[]`
* DoD：

    * 支持 ta4j/custom 引擎（沿用现有 engine 概念）
    * 能根据 `min_required_bars` 判定 valid/invalid（definition 里已有字段）
    * 返回结构支持单值/多值（对齐 value/extra_values 的语义）

### C2. 引入“缓存键规范”（效率关键）

* cacheKey 建议冻结为：

    * `indicatorCode + version + pairId + timeframe + asOfBarTime + hash(params)`
* DoD：

    * 相同 key 多次调用只算一次（缓存命中）
    * cache TTL/容量可配置（防止用户参数组合爆炸）

### C3. 复用中间结果（计算图/feature cache）

* 例如 SMA/EMA/最高低/TR 这类基础序列做二级缓存（可选但很值）
* DoD：在 batch evaluate 中，多个指标共用同一份 bar window / feature 结果，CPU 明显下降。

---

# D. 存储层重构（最容易省资源的点）

### D1. indicator_subscription 的定位调整（保留表，但默认“不驱动计算”）

* 现状：subscription 决定要算哪些指标
* 重构后建议：

    * subscription 只对 “FACT_PERSISTED 白名单指标”生效（平台级）
    * 用户自定义参数的计算不走 subscription（因为参数无限组合）
* DoD：

    * 保留 subscription CRUD（兼容旧逻辑/平台白名单）
    * 取消“订阅驱动计算器”的默认执行路径

### D2. indicator_value：默认不写；仅白名单写（可选任务）

* 现有 value 表是“指标事实”且有唯一索引幂等
* 重构建议：

    * **用户自定义参数的 on-demand 结果不写 indicator_value**
    * 平台白名单指标仍可写 indicator_value（保留复盘/解释能力）
* DoD：

    * 写库逻辑必须只对 “FACT_PERSISTED” 生效
    * 不允许因为用户自定义 params 写爆库

### D3. indicator_calc_log：保留，但写入改为“按需计算日志”

* calc_log 现有设计强调“追加写入、成功失败都记录”
* DoD：

    * evaluate/evaluateBatch 每次计算（非缓存命中）写一条 calc_log
    * status 支持 SUCCESS/FAILED/SKIPPED（缓存命中可 SKIPPED 或不写，二选一）

---

# E. 兼容迁移任务（防止老接口/老模块直接挂）

### E1. 保留 GET /api/indicator/latest，但语义改造

* 现状 read-only 查询链路：`GET /api/indicator/latest`
* 重构后：

    * 若请求是白名单指标：仍可从 indicator_value 查
    * 若是用户自定义 params：走 evaluate(on-demand) 并返回（可选带缓存）
* DoD：

    * 老前端/老策略不改也能跑
    * 响应里明确 `source = CACHE/ON_DEMAND/DB`

### E2. 删除或“feature flag”关闭旧的 IndicatorCalculator 定时/事件计算

* 现状计算器流程：监听→查订阅→算→写库
* DoD：

    * 默认关闭（配置开关）
    * 灰度：只对白名单 subscription 开启

---

# F. 性能与可观测（必须，否则“谁用谁算”会被喷慢）

### F1. 指标耗时与缓存命中率 Metrics

* 指标模块总体设计里已经提到性能优化方向：缓存、批量、并行
* DoD：

    * 指标执行耗时 P95/P99
    * cache hit ratio（按 indicatorCode 聚合）
    * batch size 分布

### F2. 压测与基准

* DoD：

    * 单指标 QPS 基准（命中/不命中）
    * batch 评估吞吐提升比例
    * BarSeries append 的延迟与内存占用

---

# G. 测试任务（你们团队建议必须有，不然上线必扯皮）

### G1. 单测

* schema 校验（参数越界、必填缺失）
* min_required_bars 不足时 valid=false
* 缓存 key 一致性（同参同上下文命中）

### G2. 集成测试

* BarClosedEvent 推进 bar window 后，evaluate(asOf=最新 bar) 得到可预期结果
* evaluateBatch 与单次 evaluate 结果一致

### G3. 回归测试

* 旧接口 latest 不崩（兼容层）

---

## Step 3：建议的实施顺序（最小闭环优先）

1. **先做 B（BarSeries 输入保留但断开计算器）**
2. **再做 C（evaluate + batch + cache）**
3. **再做 E（兼容 GET latest）**
4. **最后做 D（白名单持久化与 subscription 分流）**

这样每一步都能“跑起来并可验证”，不会卡死在迁移里。

---

## 总结

* 你要的重构，本质是把现有“订阅→每根 bar 自动计算→落库”的链路（现有快速入门明确定义了这个流程）改成：**订阅只保留用于维护 K 线输入；输出值改为按需 evaluate；用缓存与 batch 保证效率；落库只对白名单指标保留“事实层”能力（value + calc_log 体系仍可复用）。**
* 上面这份清单，你可以直接拆成 Epic：**输入维护 / 运行时评估 / 缓存与批量 / 兼容迁移 / 可观测与测试 / 白名单持久化**。

